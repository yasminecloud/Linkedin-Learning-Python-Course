# -*- coding: utf-8 -*-
"""Data Analysis with Python and Pandas_chapter2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K5wx53Q1TrQ6rq8YanQGSA9rmB1s5gFp
"""

#Basic Aggregations
#The groupby()
#challenge
import pandas as pd
transactions = pd.read_csv('/content/transactions.csv', parse_dates=["date"])
transactions.head(10)

transactions.groupby(["store_nbr"])[["transactions"]].sum().sort_values(by=["transactions"], ascending=False).iloc[:10]

#Group by multiple columns
#challenge

#multiindex DataFrames
import numpy as np
import pandas as pd

transactions = pd.read_csv("/content/transactions.csv", parse_dates=["date"])
transactions["month"] = transactions["date"].dt.month
grouped = (
    transactions.groupby(["store_nbr", "month"]).agg({"transactions": ["sum", "mean"]}).sort_values(by=["month", ("transactions", "sum")], ascending=[True, False])
)
grouped.head()

grouped.loc[(3, 1)]

grouped.iloc[:, [1]]

grouped.loc[:, [("transactions", "mean")]]

grouped.droplevel(0, axis=1).reset_index()

#the agg() method challenge:

transactions = transactions.assign(
    target_pct = transactions["transactions"] / 2500,
    met_target = (transactions["transactions"] / 2500) >= 1,
    bonus_payable = ((transactions["transactions"] / 2500) >= 1) * 100,
    month = pd.to_datetime(transactions.date).dt.month,
    day_of_week = pd.to_datetime(transactions.date).dt.dayofweek,

)

transactions

transactions.groupby(["store_nbr"]).agg({"met_target": "mean", "bonus_payable":"sum"}).sort_values("bonus_payable", ascending=False)

transactions.groupby(["month"]).agg({"met_target": "mean", "bonus_payable":"sum"}).sort_values("bonus_payable", ascending=False)

transactions.groupby(["day_of_week"]).agg({"met_target": "mean", "bonus_payable":"sum"}).sort_values("bonus_payable", ascending=False)



#transform(), challenge
transactions.assign(avg_store_transactions = (transactions.groupby(["store_nbr", "day_of_week"])["transactions"].transform("mean")),
difference = lambda x: x["transactions"] - x["avg_store_transactions"]
                    )

#transform(), challenge

transactions.query("store_nbr == 25").groupby(["day_of_week"]).agg({"transactions": "mean"})

#pivot table and melt() challenge

transactions.loc[transactions["bonus_payable"] != 0].pivot_table(index="store_nbr",  columns="day_of_week", values="bonus_payable", aggfunc="sum").iloc[:10].style.background_gradient(cmap="RdYlGn", axis =1)

transactions.loc[transactions["bonus_payable"] != 0].pivot_table(
    index="store_nbr",
    columns="day_of_week",
    values="bonus_payable",
    aggfunc="sum").reset_index().melt(id_vars="store_nbr", value_name="Bonus_Payable")

#Matplotlib API and THE Plot() method

transactions.plot()

transactions.set_index("date").loc["2013-01", "transactions"].plot()

transactions.query("store_nbr == 44").set_index("date").loc["2013-01", "transactions"].plot()

transactions_44 = transactions.loc[transactions["store_nbr"] == 44,["date", "transactions"]]
transactions_44.head()

transactions_44.plot(x="date", y="transactions")

transactions.query("store_nbr in [44, 47]").pivot_table(index="date", columns ="store_nbr").droplevel(0, axis=1).plot()

#challenge

oil=pd.read_csv("/content/oil.csv")
oil.head()

oil['date'] = pd.to_datetime(oil['date'], format='%Y-%m-%d') #or corresponding date format in oil['date'] column
oil.plot(x='date', y='dcoilwtico')

#chart titles

transactions_44.set_index('date').plot(title="Store 44 Transaction 2013-2017",
                                       xlabel="year",
                                       ylabel="Daily Transactions",
                                       color="green")

#chart color
transactions.query("store_nbr in [44,47]").pivot(index='date', columns='store_nbr').droplevel(
    0, axis=1).plot(
    #color=["#187878", "#c507e0"],
    cmap='viridis', alpha=.4)

#line styles
transactions.query("store_nbr in [44,47] and date.dt.year == 2014").pivot(index='date', columns='store_nbr').droplevel(
    0, axis=1).plot(
    #color=["#187878", "#c507e0"],
    cmap='viridis', alpha=.4,
    style=["-","--"])

#Chart legends and gridlines
transactions.query("store_nbr in [44,47] and date.dt.year == 2014").pivot(index='date', columns='store_nbr').droplevel(
    0, axis=1).plot(title="Store 44 Transaction 2013-2017",
    #color=["#187878", "#c507e0"],
    cmap='viridis', alpha=.4,
    style=["-","--"], grid=True).legend(#bbox_to_anchor=(1, 1)
    loc="upper left")

#Chart styles

import seaborn as sns
sns.set_style("darkgrid")

transactions.query("store_nbr in [44,47] and date.dt.year == 2014").pivot(index='date', columns='store_nbr').droplevel(
    0, axis=1).plot(title="Store 44 Transaction 2013-2017",
    #color=["#187878", "#c507e0"],
    cmap='viridis', alpha=.4,
    style=["-","--"], grid=True).legend(#bbox_to_anchor=(1, 1)
    loc="upper left")

import matplotlib
matplotlib.style.use("fivethirtyeight")

transactions.query("store_nbr in [44,47] and date.dt.year == 2014").pivot(index='date', columns='store_nbr').droplevel(
    0, axis=1).plot(title="Store 44 Transaction 2013-2017",
    #color=["#187878", "#c507e0"],
    cmap='viridis', alpha=.4,
    style=["-","--"], grid=True).legend(#bbox_to_anchor=(1, 1)
    loc="upper left")

#challenge

import pandas as pd
import numpy as np

oil= pd.read_csv("/content/oil.csv")

oil["date"] = pd.to_datetime(oil["date"])

oil.head()

import seaborn as sns
sns.set_style("darkgrid")
oil= oil.rename(columns={"dcoilwtico":"oil Price"})
oil

oil.set_index(oil["date"].astype("datetime64[ns]")).plot(
    title="Oil Price 2013-2017",
    xlabel="Date",
    ylabel="USD Price",
    c="Black"
)

#subplots

import numpy as np
import pandas as pd

transactions = pd.read_csv("/content/transactions.csv")
transactions

import matplotlib
matplotlib.style.use("fivethirtyeight")

# Convert 'date' column to datetime
transactions["date"] = pd.to_datetime(transactions["date"])
transactions.query("store_nbr in [44,47] and date.dt.year == 2014").pivot(index='date', columns='store_nbr').droplevel(
    0, axis=1).plot(title="Store 44 Transaction 2013-2017",
    #color=["#187878", "#c507e0"],
    cmap='viridis', alpha=.4,
    style=["-","--"], grid=True, subplots=True, layout=(1, 2), sharey=True,figsize=(10, 10)).legend(#bbox_to_anchor=(1, 1)
    loc="upper left")

#challenge
stores_1234 = (transactions.loc[transactions["store_nbr"].isin([1,2,3,4])].
    pivot_table(index="date", columns="store_nbr").droplevel(0, axis=1))

stores_1234

stores_1234.plot(subplots=True, layout=(1,4), figsize=(8,8), sharey=True)

#Bar Charts
store_40s=list(range(40,50))

transactions.query("store_nbr in @store_40s").groupby(["store_nbr"]).agg({"transactions": "sum"}).sort_values(by="transactions").plot.bar(figsize=(10, 10))
#barh if you want the bars horizantal

#Grouped Bar Charts

#Grouped and Stacked bar charts
store_40s=list(range(40,50))

transactions.query("store_nbr in @store_40s and date.dt.month in [1, 2, 3]").pivot_table(index=transactions["date"].dt.month,
                                                                                         columns= transactions["store_nbr"],
                                                                                         values="transactions",
                                                                                         aggfunc="sum")#.plot.barh(stacked=True).legend(bbox_to_anchor=(1, 1))

#Grouped and Stacked bar charts
store_40s=list(range(40,50))

transactions.query("store_nbr in @store_40s and date.dt.month in [1, 2, 3]").pivot_table(index=transactions["date"].dt.month,
                                                                                         columns= transactions["store_nbr"],
                                                                                         values="transactions",
                                                                                         aggfunc="sum").apply(lambda x: x*100/ sum(x), axis=1).plot.barh(stacked=True).legend(bbox_to_anchor=(1, 1))

#Challenge Bar charts

#challenge
stores_1234 = (transactions.loc[transactions["store_nbr"].isin([1,2,3,4])].
    pivot_table(index="date", columns="store_nbr").droplevel(0, axis=1))

stores_1234

stores_1234.sum().sort_values(ascending=False).plot.bar()

# Convert 'date' column to datetime
transactions["date"] = pd.to_datetime(transactions["date"])
stores_1234.index = stores_1234.index.astype("datetime64[ns]")

stores_1234_monthly = stores_1234.groupby(stores_1234.index.month).sum()

stores_1234_monthly.plot.bar(stacked= True).legend(bbox_to_anchor=(1,1))

#Pie charts and Scatterplots
transactions.head()

store_40s=list(range(40,50))
transactions.query("store_nbr in @store_40s and date.dt.month in [1,2,3]").groupby(
    "store_nbr").agg({"transactions": "sum"}).plot.pie(y="transactions").legend(bbox_to_anchor=(1,1))

transactions.query("store_nbr in @store_40s").pivot_table(index=transactions["date"].dt.month,
                                                         columns="store_nbr",
                                                         values="transactions",
                                                         aggfunc="sum").plot.scatter(x=44, y = 47)

#challenge scatterplots

#subplots

import numpy as np
import pandas as pd

transactions = pd.read_csv("/content/transactions.csv")
transactions

# Convert 'date' column to datetime
transactions["date"] = pd.to_datetime(transactions["date"])

#challenge
stores_1234 = (transactions.loc[transactions["store_nbr"].isin([1,2,3,4])].
    pivot_table(index="date", columns="store_nbr").droplevel(0, axis=1))

stores_1234

stores_1234.plot.scatter(x=3, y=2)

stores_1234.plot.scatter(x=3, y=2, c=stores_1234.index.month, colormap="Set2")

stores_1234.sum().sort_values(ascending=True).plot.pie(startangle=90)

#Histograms
transactions.loc[transactions["store_nbr"] == 47,"transactions"].plot.hist()

#challenge Histograms

stores_1234.loc[:, [2,3]].plot.hist(alpha=.3)

##Mid-course Project

import numpy as np
import pandas as pd

transactions=pd.read_csv("/content/project_transactions.csv", dtype={"DAY": "Int16",
                                                                     "QUANTITY": "Int32",
                                                                     "STORE_ID":"Int32",

                                                                     "WEEK_NO":"Int8"})
transactions

#Reduced Memory usage by -35MB afetr converting to correct dtypes
transactions.info(memory_usage="deep")

#use to identify which values can be downcast
transactions.describe().round()

transactions.isna().sum()

#calculate unique househols in dataset with nunique(describe could also be used)
transactions["household_key"].nunique()

#calculate unique product__ids in dataset with nunique
transactions["PRODUCT_ID"].nunique()

#create a discount sum column and a percentage discount column

transactions = (transactions.assign(total_discount = transactions["RETAIL_DISC"]+ transactions["COUPON_DISC"],
                  percentage_discount = (lambda x: (x["total_discount"] / x["SALES_VALUE"]).abs())).drop(["RETAIL_DISC", "COUPON_DISC", "COUPON_MATCH_DISC"], axis=1))
transactions.head()

#use where to cap values above 1 at 1 and below 0 at 0. other methods could be used here as well

transactions["percentage_discount"]=(transactions["percentage_discount"]
                                     .where(transactions["percentage_discount"]< 1, 1.0)
                                     .where(transactions["percentage_discount"]> 0, 0))
transactions.head()

# total sales value
transactions['SALES_VALUE'].sum()

#total discount
transactions["total_discount"].sum()

#overall percent discount

transactions['total_discount'].sum()/transactions['SALES_VALUE'].sum()

#average of pct_discount

transactions["percentage_discount"].mean()

#average of pct_discount column

transactions["percentage_discount"].mean()

#total quantity sold
transactions["QUANTITY"].sum()

#max quantity in single row

transactions['QUANTITY'].max()

#use to grab row with max value - discount rate is lower than average
transactions.loc[transactions["QUANTITY"].argmax()]

#sales value per transaction/basket
transactions['SALES_VALUE'].sum() / transactions['BASKET_ID'].nunique()

#sales value per household
transactions['SALES_VALUE'].sum() / transactions['household_key'].nunique()

#plot distribution of houuseholds by total sales value
#First groupby household and calculate sum of sales
#the plot with a histogram

(transactions.groupby("household_key").agg({"SALES_VALUE": "sum"}).plot.hist())

#store top 10 households by total value and quantity
#groupby household_key, calculate sum of relevant columns by household
#sort both by relevant metric in descending order, grab top 10 rows

top10_value =(transactions.groupby("household_key").agg({"SALES_VALUE": "sum"}).sort_values("SALES_VALUE", ascending = False).iloc[:10])
top10_value

top10_quant = (transactions.groupby("household_key").agg({'QUANTITY': 'sum'}).sort_values("QUANTITY", ascending=False).iloc[:10])
top10_quant

#use multiple aggregation to create both in a simple table an option
#this here is just to use to comapare to chart
(transactions.groupby("household_key").agg({"SALES_VALUE": "sum", "QUANTITY": "sum"}).sort_values("SALES_VALUE", ascending=False).loc[:,"SALES_VALUE"].describe())

#top 10 households by sales value plotted with a bar plot
top10_value["SALES_VALUE"].plot.bar()

#create top 10 products by sales
#group by PRODUCT_ID and sum sales value by product
#Sort in descending order and grab top 10 rows

top10_products = (transactions.groupby(["PRODUCT_ID"]).agg({"SALES_VALUE": "sum"}).sort_values("SALES_VALUE", ascending=False).iloc[:10])
top10_products

#plot top 10 products by sale value

top10_products["SALES_VALUE"].sort_values().plot.barh()

#Calculate the total discount for top 10 products
#Divide that by sales value for top 10 products
((transactions.query("PRODUCT_ID in @top10_products.index").loc[: ,"total_discount"].sum())/(transactions.query("PRODUCT_ID in @top10_products.index").loc[:,"SALES_VALUE"].sum()))

#read in products data

products = pd.read_csv("/content/product.csv")
products.head()

#look up top 10 products for households in top10_value table
#Use query to reference index of top10_value to filter to relevant households
#Use value counts to get counts by product_id (this will be order in descending order)
#then grab the top 10 products with iloc and extract the index to get product numbers

top_hh_products =(transactions.query("household_key in @top10_value.index").loc[:, "PRODUCT_ID"].value_counts().iloc[:10].index)

top_hh_products

#Filter product table to products from prior cell
products.query("PRODUCT_ID in @top_hh_products")

#PRODUCT WITH HIGHEST QUANTITY IN A SINGLE ROW

products.query("PRODUCT_ID == 6534178")

#look up 10 product names for all customers (from first cell)

products.query("PRODUCT_ID in @top10_products.index")

#Data Analysis & Pandas

from datetime import datetime
now = datetime.now()
now

import pandas as pd
retail = pd.read_csv("/content/retail_2016_2017.csv")
retail.head()

retail.info(memory_usage="deep")

retail = retail.astype({"date": "datetime64[ns]"}) #Change datetime64 to datetime64[ns] to specify nanosecond precision
retail.head()

retail.info(memory_usage="deep")

retail.loc[0, ["date"]] = 'N/A'
retail.head()

retail.assign(date=pd.to_datetime(retail["date"], errors="coerce" , infer_datetime_format = True)).dtypes

#Formatting dates
import pandas as pd
retail = pd.read_csv("/content/retail_2016_2017.csv")
retail.head()

retail["date"] = pd.to_datetime(retail["date"])

retail.info(memory_usage="deep")

retail["date"].dt.strftime("%Y").head()

retail["date"].dt.strftime("%Y-%b-%d").head()

#extracting Datetime Components
retail.assign(year=retail["date"].dt.month)
#retail.assign(year=retail["date"].dt.dayofweek)
#retail.assign(year=retail["date"].dt.year)

#assignment
import pandas as pd
import numpy as np
transactions = pd.read_csv("/content/transactions.csv", parse_dates=["date"])
transactions.head()

transactions["date"].max()

#difference between date and max date
transactions["time_to_last_date"] = transactions["date"].max() - transactions["date"]

#Dateparts
transactions["year"] = transactions["date"].dt.year
transactions["month"] = transactions["date"].dt.month
transactions["day_of_week"] = transactions["date"].dt.weekday

#Format Date
transactions["date"] = transactions["date"].dt.strftime("%Y-%B-%d")
transactions.head()

#time deltas and arithmetic
retail = pd.read_csv("/content/retail_2016_2017.csv", parse_dates=["date"])
retail.head()

retail.info(memory_usage="deep")

retail = retail.assign(birthday=pd.to_datetime("1950-01-01"))
retail.head()

retail=retail.assign(age=(retail["date"]- retail["birthday"]).dt.days / 365.25)
retail.head()

retail = retail.assign(retirement_date = retail["date"] + (pd.to_timedelta(365.25, unit="D")*20))
retail.head()

#assignment: time deltas
import pandas as pd
import numpy as np
transactions = pd.read_csv("/content/transactions.csv", parse_dates=["date"])
transactions.tail()

transactions = transactions.assign(year=transactions["date"].dt.year,
month=transactions["date"].dt.month,
day_of_week=transactions["date"].dt.dayofweek,
time_to_last_date=transactions["date"].max() - transactions["date"],)
transactions.head()

transactions.assign(time_to_last_date=pd.to_timedelta(3, unit="W") + transactions["time_to_last_date"],
                    weeks_to_last_date=(lambda x: x["time_to_last_date"].dt.days / 7),
                    ).head()

import numpy as np
import pandas as pd

retail = pd.read_csv("/content/retail_2016_2017.csv", parse_dates=["date"])
retail.head()

retail_agg = retail.groupby(['date']).agg({"sales":"sum"}).round().iloc[:10]
retail_agg

retail_agg.iloc[1]= np.nan
retail_agg.iloc[4:6] = np.nan
retail_agg

retail_agg.plot()

retail_agg.fillna(retail_agg["sales"].mean()).plot()

retail_agg.ffill().plot()

retail_agg.bfill().plot()

retail_agg.interpolate().plot()

#assignment

oil=pd.read_csv("/content/oil.csv", index_col="date", parse_dates=True)
oil.head()

oil.index.dtype

oil.mean()

oil.plot()

oil["dcoilwtico"].loc["2014"].bfill().plot()

oil.loc["2014-12"].ffill().plot()

oil.loc["2014-12-01":"2014-12-15"].interpolate().plot()

#shifting time series

import pandas as pd
import numpy as np
retail = pd.read_csv("/content/retail_2016_2017.csv", parse_dates=["date"])
retail.head()

retail_agg=retail.groupby(['date']).agg({"sales": "sum"}).round().iloc[1:10]
retail_agg.assign(yesterday_sales = retail_agg["sales"].shift())

#Diff()
retail_agg.assign(daily_change = (retail_agg["sales"].diff() / retail_agg["sales"].shift()))

#assignment shift() & diff()
transactions = pd.read_csv("/content/transactions.csv")
transactions.head()

#shifting time series

import pandas as pd
import numpy as np
retail = pd.read_csv("/content/retail_2016_2017.csv", parse_dates=["date"])
retail.head()

transactions_47 = transactions.loc[transactions["store_nbr"] == 47, ["date", "transactions"]]
transactions_47["date"] = pd.to_datetime(transactions_47["date"])

# Group by year and month from filtered DataFrame
transactions_47 = transactions_47.groupby([transactions_47["date"].dt.year, transactions_47["date"].dt.month])["transactions"].sum()

# Optional: Rename index for clarity
transactions_47.index.names = ["year", "month"]

transactions_47["year_prior"] = transactions_47.shift(12)

transactions_47.head()

transactions_47 = transactions.loc[transactions["store_nbr"] == 47, ["date", "transactions"]]
transactions_47["date"] = pd.to_datetime(transactions_47["date"])

# Group by year and month from filtered DataFrame
transactions_47 = transactions_47.groupby([transactions_47["date"].dt.year, transactions_47["date"].dt.month])["transactions"].sum().to_frame()
#we add to frame to make series a full dataframe, at that time we can add columns
# Optional: Rename index for clarity
transactions_47.index.names = ["year", "month"]

# Apply shift directly to the Series values
transactions_47["year_prior"] = transactions_47.shift(12)


transactions_47.head(24)

transactions_47.loc[2015].plot()

#Aggregation & Resampling

import pandas as pd
import numpy as np
retail = pd.read_csv("/content/retail_2016_2017.csv", parse_dates=["date"])
retail.head()

# Group by year and month, then sum the 'sales' column
retail_agg = retail.groupby([retail["date"].dt.year, retail["date"].dt.month])["sales"].agg("sum").round(2)# Group by year and month, then sum the 'sales' column
retail_agg

# Create a datetime index from the current MultiIndex
retail_agg.index = pd.to_datetime(retail_agg.index.map(lambda x: f'{x[0]}-{x[1]}-01')) # Assuming the aggregation is by month, set the day to 01
retail_agg.index.name = 'date' # Name the index for clarity


# Now you can resample the DataFrame
retail_agg.resample("M").sum().round(2)

# calculate the sum by quarter
retail_agg.resample("Q").sum().round(2)

#assignment: Resampling

import numpy as np
import pandas as pd
oil= pd.read_csv("/content/oil.csv", index_col="date", parse_dates=True)
oil.head()

oil.resample("Y").mean().plot()

for period in ["D","W", "M", "A"]:
  oil.resample(period).mean().plot()

#Aggregation & Resampling

import pandas as pd
import numpy as np
retail = pd.read_csv("/content/retail_2016_2017.csv", parse_dates=["date"])
retail.head()

retail_daily = retail.set_index("date").loc[:, "sales"].resample("D").sum().round(2)
retail_daily

retail_daily.rolling(90).mean().plot()

retail_daily.rolling(30).mean().plot()

retail_daily.rolling(7).mean().plot()

#assignment: Rolling aggregations

import pandas as pd
import numpy as np
transactions = pd.read_csv("/content/transactions.csv", parse_dates=["date"])
transactions.head()

transactions_47 = (transactions.loc[transactions["store_nbr"]==47, ["date", "transactions"]].set_index("date"))
transactions_47.head()

(transactions_47.assign(transactions_rolling_avg_90 = transactions_47.rolling(90).mean()).drop(["transactions"], axis=1).plot())

transactions_47.plot()

#Preprocessing with read_csv()

import pandas as pd
import numpy as np
columns=["index", "Date", "Oil Price"]
oil = pd.read_csv("/content/oil.csv", header=None, names=columns)


oil.head()

pd.read_csv("/content/oil_no_header.csv", header=None)

#Column Selection
columns =["Date", "Oil Price"]
oil = pd.read_csv("/content/oil.csv", header=None, names=columns, index_col = "Date", parse_dates=True)
oil.index.dtype
oil.head()

oil.head()

#Row selection & missing values
import pandas as pd
columns = ["Date", "Oil Price"]

oil = pd.read_csv("/content/oil.csv", header=0, names=columns, index_col= "Date", parse_dates=True, skiprows=[0], nrows=3)
oil.head()

#Parsing dates & Data Types
import pandas as pd
columns = ["Date", "Oil Price", "Date2"]

oil = pd.read_csv("/content/oil.csv", names=columns, parse_dates=["Date","Date2"], index_col="Date", dtype={"index": "Int16"})

oil.head()

#Converters

#Parsing dates & Data Types
import pandas as pd
columns = ["Date", "Oil Price", "Date2"]
string_slice = lambda x: int(x[-2:])

oil = pd.read_csv("/content/oil.csv", names=columns, parse_dates=["Date","Date2"], index_col="Date", dtype={"index": "Int16"}, converters= {"Date2": string_slice})

oil.head()

#challenge: Importing Data
import pandas as pd
import numpy as np
transactions = pd.read_csv("/content/transactions.csv",
                           # Explicitly provide column names as done in a previous cell
                           header=None,
                           names=["Date", "Store_Number", "Transaction_Count"],
                           skiprows=[0], # Skip the original header row if it exists
                           parse_dates=["Date"],
                           dtype={"Store_Number":"Int8", "Transaction_Count": "Int16"}).assign(
                             target_pct = lambda x: (x["Transaction_Count"] / 2500),
                              met_target = lambda x: (x["Transaction_Count"] / 2500 >= 1),
                               # Corrected bonus_payable calculation: removed the extra 1*
                              bonus_payable = lambda x: (x["Transaction_Count"] / 2500 >= 1 )
                              * 100,
                               month = lambda x: x["Date"].dt.month,
                               day_of_week = lambda x : x["Date"].dt.dayofweek,
                           ).astype({"target_pct": "Float32",
                                     "month": "Int8",
                                     "day_of_week":"Int8"})

transactions.head()

transactions = pd.read_csv("/content/transactions.csv",
                           header=0,
                           names=["Date", "Store_Number", "Transaction_Count"],
                           skiprows=[0],
                           parse_dates=["Date"],
                           dtype={"Store_Number":"Int8", "Transaction_Count": "Int16"})
transactions.head()

transactions.tail()

#assignment: exporting data

with pd.ExcelWriter("DataForChandler.xlsx") as writer:
    for year in range(2013, 2018):
        transactions.loc[transactions["Date"].dt.year == year].to_excel(writer, sheet_name=str(year))

for year in range(2013, 2018):
  transactions.loc[transactions["Date"].dt.year == year].to_csv(f"transactions_{year}.csv")

#Other supported files format:
import pandas as pd
url = 'https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_GDP'

gdp_data = pd.read_html(url)[0]

gdp_data.head()

gdp_data = (gdp_data.droplevel(level=0, axis=1).drop(index=0))

(gdp_data.set_index("State or federal district").iloc[:10, 0].plot.bar());

#Data Analysis with Python & Pandas
import pandas as pd
retail = pd.read_csv("/content/retail_2016_2017.csv")

retail.head()

automotive = retail.query("family == 'AUTOMOTIVE'").head()
automotive

beauty = retail.query("family=='BEAUTY'").head()
beauty

pd.concat([automotive, beauty],ignore_index=True)

#or
pd.concat([automotive, beauty]).reset_index()

#challenge: Appending DataFrames

transactions = pd.concat((pd.read_csv("transactions_2014.csv"), pd.read_csv("transactions_2015.csv"))).drop(['Unnamed: 0'], axis=1)

transactions.head()

transactions_2015 = transactions.query("date =='2015'")
transactions_2015

pd.concat([transactions_2014, transactions_2015])

#Joining Dataframes

import pandas as pd
transactions = pd.read_csv("/content/transactions.csv", parse_dates=["date"])
oil = pd.read_csv("/content/oil.csv", parse_dates=["date"])

transactions.info()

oil.info()

oil.info()

#inner join
transactions.merge(oil,
                   how="inner",
                   left_on=["date"],
                   right_on=["date"])

#left join
transactions.merge(oil,
                   how="left",
                   left_on=["date"],
                   right_on=["date"]
                   ).loc[transactions["date"] == "2013-07-06"]

transactions.merge(oil,
                   how="outer",
                   left_on=["date"],
                   right_on=["date"])

transactions.tail()

#challenge: Joining DataFrames
stores = pd.read_csv("/content/stores.csv")
retail = pd.read_csv("/content/retail_2016_2017.csv", parse_dates=["date"])

retail.head()

stores.head()

retail_stores=retail.merge(stores, how="left",
             left_on="store_nbr",
             right_on="store_nbr")
retail_stores.head()

(retail_stores.groupby("city").agg({"sales": "sum"}).sort_values(by="sales", ascending=False).plot.bar())

retail_stores.pivot_table(
  index=["date"] ,
  columns="type",
  values="sales",
  aggfunc="sum"
).plot(figsize=(10, 10))

retail_stores["date"] = pd.to_datetime(retail_stores["date"])

retail_stores.pivot_table(
    index="type",
    columns=retail_stores["date"].dt.month,
    values="sales",
    aggfunc="mean"
).T.plot.bar(stacked=True).legend(bbox_to_anchor=(1,1))

#The final project
import pandas as pd
import numpy as np

transactions = pd.read_csv("/content/project_transactions.csv")
transactions.head()

cols = ["household_key", "BASKET_ID", "DAY", "PRODUCT_ID", "QUANTITY", "SALES_VALUE"]
transactions.loc[:, cols].describe().round()

#Specify datatypes to convert
dtypes={"DAY":"Int16", "QUANTITY":"Int32", "PRODUCT_ID":"Int32"}
transactions = pd.read_csv("/content/project_transactions.csv", dtype=dtypes, usecols=cols + ["DAY"])

#use the following snippet to create a date column then drop. then drop the "day" and "week_no" columns.

transactions = (transactions.assign(date=(pd.to_datetime("2016", format='%Y') + pd.to_timedelta(transactions["DAY"].sub(1).astype(str) + "days"))).drop(["DAY"], axis=1))


transactions.head()

product = pd.read_csv("/content/product.csv")

product.info(memory_usage="deep")
product.head()

demographics=pd.read_csv("/content/hh_demographic.csv")

demographics.head()

#Set a date index, graby the sales column, and calculate a monthly sum using resampling
#then build the default line plot

(transactions.set_index("date").loc[:, "SALES_VALUE"].resample("M").sum().plot())

##Filter above plot to specified date range with row slice in .loc
(transactions.set_index("date").sort_index().loc["2016-04":"2017-10", "SALES_VALUE"].resample("ME").sum().plot())



